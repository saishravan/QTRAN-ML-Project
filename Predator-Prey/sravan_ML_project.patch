diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..e69de29
diff --git a/.idea/.gitignore b/.idea/.gitignore
new file mode 100644
index 0000000..26d3352
--- /dev/null
+++ b/.idea/.gitignore
@@ -0,0 +1,3 @@
+# Default ignored files
+/shelf/
+/workspace.xml
diff --git a/.idea/QTRAN-ML-Project.iml b/.idea/QTRAN-ML-Project.iml
new file mode 100644
index 0000000..86d1115
--- /dev/null
+++ b/.idea/QTRAN-ML-Project.iml
@@ -0,0 +1,12 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module type="PYTHON_MODULE" version="4">
+  <component name="NewModuleRootManager">
+    <content url="file://$MODULE_DIR$" />
+    <orderEntry type="jdk" jdkName="Python 2.7" jdkType="Python SDK" />
+    <orderEntry type="sourceFolder" forTests="false" />
+  </component>
+  <component name="PyDocumentationSettings">
+    <option name="format" value="PLAIN" />
+    <option name="myDocStringFormat" value="Plain" />
+  </component>
+</module>
\ No newline at end of file
diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
new file mode 100644
index 0000000..105ce2d
--- /dev/null
+++ b/.idea/inspectionProfiles/profiles_settings.xml
@@ -0,0 +1,6 @@
+<component name="InspectionProjectProfileManager">
+  <settings>
+    <option name="USE_PROJECT_PROFILE" value="false" />
+    <version value="1.0" />
+  </settings>
+</component>
\ No newline at end of file
diff --git a/.idea/misc.xml b/.idea/misc.xml
new file mode 100644
index 0000000..7ba73c2
--- /dev/null
+++ b/.idea/misc.xml
@@ -0,0 +1,4 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 2.7" project-jdk-type="Python SDK" />
+</project>
\ No newline at end of file
diff --git a/.idea/modules.xml b/.idea/modules.xml
new file mode 100644
index 0000000..49db8ec
--- /dev/null
+++ b/.idea/modules.xml
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectModuleManager">
+    <modules>
+      <module fileurl="file://$PROJECT_DIR$/.idea/QTRAN-ML-Project.iml" filepath="$PROJECT_DIR$/.idea/QTRAN-ML-Project.iml" />
+    </modules>
+  </component>
+</project>
\ No newline at end of file
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
new file mode 100644
index 0000000..94a25f7
--- /dev/null
+++ b/.idea/vcs.xml
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="VcsDirectoryMappings">
+    <mapping directory="$PROJECT_DIR$" vcs="Git" />
+  </component>
+</project>
\ No newline at end of file
diff --git a/Predator-Prey/agent.log b/Predator-Prey/agent.log
new file mode 100644
index 0000000..3ddab48
--- /dev/null
+++ b/Predator-Prey/agent.log
@@ -0,0 +1,336 @@
+[INFO|main.py:42] 2020-05-24 18:47:57,120 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-24 18:47:57,474 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-24 18:47:57,475 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-24 19:23:17,666 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-24 19:24:08,415 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-24 19:24:11,179 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-24 19:35:06,221 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-24 19:35:13,549 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-24 19:35:14,800 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-24 20:09:47,390 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-24 20:09:47,427 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-24 20:09:47,428 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-24 20:27:03,133 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-24 20:27:03,170 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-24 20:27:03,170 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-24 20:29:39,448 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-24 20:29:39,484 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-24 20:29:39,485 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 04:11:48,736 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 04:11:48,966 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 04:11:48,967 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 04:20:07,516 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 04:21:09,105 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 04:21:28,584 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 04:38:40,561 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 04:38:40,628 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 04:38:40,629 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 04:42:34,417 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 04:42:34,460 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 04:42:34,460 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 04:48:46,952 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 04:48:46,995 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 04:48:46,995 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 04:49:42,703 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 04:49:42,759 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 04:49:42,759 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 05:12:33,158 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 05:12:33,193 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 05:12:33,194 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 05:13:00,967 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 05:13:01,004 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 05:13:01,004 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 06:22:32,497 > [Agent] Agent: cac_fo
+[INFO|main.py:42] 2020-05-26 11:32:12,741 > [Agent] Agent: pos_cac_fo
+[INFO|main.py:42] 2020-05-26 11:32:54,854 > [Agent] Agent: pos_cac_fo
+[INFO|main.py:42] 2020-05-26 11:35:33,044 > [Agent] Agent: pos_cac_fo
+[INFO|main.py:42] 2020-05-26 11:36:21,253 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 11:36:21,317 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 11:36:21,317 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 11:38:26,348 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 11:38:26,385 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 11:38:26,385 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 11:49:26,850 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 11:49:26,889 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 11:49:26,890 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 11:54:48,560 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 11:54:48,601 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 11:54:48,602 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 11:57:16,253 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 11:57:16,291 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 11:57:16,291 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 12:02:13,323 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 12:02:13,361 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 12:02:13,361 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 12:02:50,124 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 12:02:50,160 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 12:02:50,160 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 12:07:48,256 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 12:07:48,297 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 12:07:48,297 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 12:19:15,142 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:41] 2020-05-26 12:19:15,181 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 12:19:15,182 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 13:03:30,624 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 13:03:30,662 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 13:03:30,663 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 13:20:35,122 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 13:20:35,160 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 13:20:35,160 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 15:32:44,662 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 15:32:44,700 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 15:32:44,701 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 15:38:02,690 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 15:38:02,729 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:32] 2020-05-26 15:38:02,729 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 15:50:11,273 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 15:50:11,336 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 15:50:11,337 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 16:08:43,149 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 16:08:43,187 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 16:08:43,188 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 16:48:50,342 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 16:48:50,383 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 16:48:50,383 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 16:53:40,607 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 16:53:40,645 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 16:53:40,645 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 16:57:18,659 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 16:57:18,695 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 16:57:18,695 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 17:39:32,005 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 17:39:32,041 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 17:39:32,041 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 17:44:00,929 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 17:44:00,967 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 17:44:00,967 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 18:00:55,445 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 18:00:55,483 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 18:00:55,483 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 18:10:31,250 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 18:10:31,287 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 18:10:31,287 > [Agent] Centralized DQN Agent
+[INFO|main.py:42] 2020-05-26 18:57:48,061 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-26 18:57:48,128 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-26 18:57:48,128 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 14:49:51,207 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 14:49:51,870 > [Agent] Centralized DQN Trainer is created
+[INFO|main.py:51] 2020-05-27 14:51:16,061 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 14:51:16,100 > [Agent] Centralized DQN Trainer is created
+[INFO|main.py:51] 2020-05-27 14:51:30,911 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 14:51:30,952 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 14:51:30,952 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 14:52:43,745 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 14:52:43,784 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 14:52:43,784 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 14:56:50,818 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 14:56:50,864 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 14:56:50,864 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 14:58:59,418 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 14:58:59,497 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 14:58:59,497 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 15:01:06,736 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 15:01:06,774 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 15:01:06,774 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 15:09:22,627 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 15:09:22,666 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 15:09:22,667 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 15:14:34,322 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 15:14:34,359 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 15:14:34,359 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 15:17:48,471 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 15:17:48,509 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 15:17:48,509 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 16:13:36,099 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 16:13:36,170 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 16:13:36,170 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 16:56:16,729 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 16:56:16,770 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 16:56:16,771 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 17:07:44,482 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 17:07:44,521 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 17:07:44,521 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 17:24:15,790 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 17:24:15,833 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 17:24:15,833 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 17:43:35,553 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 17:43:35,590 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 17:43:35,591 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 17:46:24,223 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 17:46:24,261 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 17:46:24,261 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 19:01:47,990 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 19:01:48,028 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 19:01:48,028 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 19:04:45,756 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 19:04:45,794 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 19:04:45,794 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 19:15:46,377 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 19:15:46,415 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 19:15:46,416 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 19:26:22,388 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 19:26:22,426 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 19:26:22,426 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 19:36:26,357 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 19:36:26,398 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 19:36:26,398 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 19:43:38,492 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 19:43:38,531 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 19:43:38,531 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 19:49:48,960 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 19:49:49,000 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 19:49:49,001 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 19:54:36,599 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 19:54:36,638 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 19:54:36,638 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 20:27:20,024 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 20:27:20,063 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 20:27:20,063 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 20:36:55,500 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 20:36:55,541 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 20:36:55,541 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 20:56:18,294 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 20:56:18,415 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 20:56:18,415 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 21:00:21,494 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 21:00:21,559 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 21:00:21,560 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-27 21:10:30,834 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-27 21:10:30,873 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-27 21:10:30,873 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 08:03:43,302 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-28 08:03:43,343 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 08:03:43,344 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 08:07:21,040 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:42] 2020-05-28 08:07:21,080 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 08:07:21,081 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 08:17:29,036 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 08:17:29,075 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 08:17:29,076 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 08:19:14,872 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 08:19:14,914 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 08:19:14,914 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 08:26:57,547 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 08:26:57,589 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 08:26:57,590 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 08:32:23,938 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 08:32:23,980 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 08:32:23,980 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 10:06:33,766 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 10:06:33,810 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 10:06:33,810 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 10:20:25,917 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 10:20:25,958 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 10:20:25,958 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 10:25:25,990 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 10:25:26,030 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 10:25:26,031 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 10:36:16,520 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 10:36:16,561 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 10:36:16,561 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 10:40:02,196 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 10:40:02,237 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 10:40:02,238 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 10:58:58,142 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 10:58:58,182 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 10:58:58,182 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 11:10:34,246 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 11:10:34,290 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 11:10:34,290 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 11:11:54,446 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 11:11:54,487 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 11:11:54,487 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 11:35:03,556 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 11:35:03,597 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 11:35:03,597 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 11:35:59,035 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 11:35:59,080 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 11:35:59,080 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 11:49:12,054 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 11:49:12,095 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 11:49:12,095 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 12:09:21,854 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 12:09:21,897 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 12:09:21,898 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 12:10:21,484 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 12:10:21,542 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 12:10:21,543 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 12:16:28,718 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 12:16:28,761 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 12:16:28,762 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 12:18:40,332 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 12:18:40,375 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 12:18:40,375 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 12:30:52,501 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 12:30:52,541 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 12:30:52,541 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 12:32:42,126 > [Agent] Agent: pos_cac_fo
+[INFO|main.py:51] 2020-05-28 12:33:09,705 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 12:33:09,745 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 12:33:09,745 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 12:51:34,288 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 12:51:34,349 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 12:51:34,350 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 12:53:46,248 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 12:53:46,294 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 12:53:46,294 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 12:56:16,802 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 12:56:16,842 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 12:56:16,843 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 14:16:27,938 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 14:16:28,214 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 14:16:28,214 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 14:18:50,125 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 14:18:50,169 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 14:18:50,170 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 14:46:59,378 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 14:46:59,424 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 14:46:59,424 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 15:10:28,833 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 15:10:28,874 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 15:10:28,875 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 16:26:06,730 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 16:26:06,773 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 16:26:06,773 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 16:27:13,213 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 16:27:13,255 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 16:27:13,256 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 16:47:31,053 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 16:47:31,095 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 16:47:31,095 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 17:04:50,161 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 17:04:50,292 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 17:04:50,292 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 17:34:31,659 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 17:34:31,703 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 17:34:31,703 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 19:29:36,410 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 19:29:36,456 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 19:29:36,456 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 19:40:43,850 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 19:40:43,896 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 19:40:43,896 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 19:43:49,487 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 19:43:49,530 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 19:43:49,530 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 21:41:40,985 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 21:41:41,031 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 21:41:41,032 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 23:51:40,582 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 23:51:40,664 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 23:51:40,664 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-28 23:59:00,471 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-28 23:59:00,518 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-28 23:59:00,519 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-29 06:56:40,745 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-29 06:56:40,916 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-29 06:56:40,917 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-29 07:01:40,440 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-29 07:01:40,483 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-29 07:01:40,483 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-29 07:04:52,746 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-29 07:04:52,794 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-29 07:04:52,795 > [Agent] Centralized DQN Agent
+[INFO|main.py:51] 2020-05-29 07:18:28,060 > [Agent] Agent: pos_cac_fo
+[INFO|trainer.py:43] 2020-05-29 07:18:28,437 > [Agent] Centralized DQN Trainer is created
+[INFO|agent.py:33] 2020-05-29 07:18:28,437 > [Agent] Centralized DQN Agent
diff --git a/Predator-Prey/agents/config_agents.py b/Predator-Prey/agents/config_agents.py
index a7f0c4d..814d2cf 100644
--- a/Predator-Prey/agents/config_agents.py
+++ b/Predator-Prey/agents/config_agents.py
@@ -6,21 +6,22 @@
 def config_agent(_flags):
     flags = _flags
 
-    flags.DEFINE_string("agent", "cac_fo", "Agent")
-
-    flags.DEFINE_integer("training_step", 500000, "Training time step")
-    flags.DEFINE_integer("testing_step", 1000, "Testing time step")
-    flags.DEFINE_integer("max_step", 200, "Maximum time step per episode")
-    flags.DEFINE_integer("eval_step", 1000, "Number of steps before training")
+    #flags.DEFINE_string("agent", "cac_fo", "Agent")
+    flags.DEFINE_string("agent", "pos_cac_fo", "Agent")
+    #flags.DEFINE_integer("training_step", 500000, "Training time step")
+    flags.DEFINE_integer("training_step", 3000000, "Training time step: Total number of time steps during training")
+    flags.DEFINE_integer("testing_step", 10000, "Testing time step: Total number of time steps during each test")
+    flags.DEFINE_integer("max_step", 100, "Maximum time step per episode: Maximum number of time steps in each episode")
+    flags.DEFINE_integer("eval_step", 10, "Number of steps before training: Number of episodes before testing again")
     # flags.DEFINE_integer("training_step", 5000, "Training time step")
     # flags.DEFINE_integer("testing_step", 1000, "Testing time step")
     # flags.DEFINE_integer("max_step", 200, "Maximum time step per episode")
     # flags.DEFINE_integer("eval_step", 1000, "Number of steps before training")
 
-    flags.DEFINE_integer("b_size", 10000, "Size of the replay memory")
-    flags.DEFINE_integer("m_size", 32, "Minibatch size")
-    flags.DEFINE_integer("pre_train_step", 300, "during [m_size * pre_step] take random action")
-    flags.DEFINE_float("lr", 0.00025, "Learning rate")
+    flags.DEFINE_integer("b_size", 1000000, "Size of the replay memory: each observation,state,action is enqueud into replay buffer")
+    flags.DEFINE_integer("m_size", 64, "Minibatch size")
+    flags.DEFINE_integer("pre_train_step", 100, "during [m_size * pre_step] take random action")
+    flags.DEFINE_float("lr", 0.0001, "Learning rate")
     # flags.DEFINE_float("lr", 0.01, "Learning rate") # it is for single
     flags.DEFINE_float("df", 0.99, "Discount factor")
 
@@ -31,7 +32,7 @@ def config_agent(_flags):
     flags.DEFINE_boolean("qtrace", False, "Use q trace")
     flags.DEFINE_boolean("kt", False, "Keyboard input test")
     flags.DEFINE_boolean("use_action_in_critic", False, "Use guided samples")
-    flags.DEFINE_string("algorithm", "ddd",
+    flags.DEFINE_string("algorithm", "pqmix7",
                     "Which agent to run, as a python path to an Agent class.")
     
 
diff --git a/Predator-Prey/agents/pos_cac_fo/agent.py b/Predator-Prey/agents/pos_cac_fo/agent.py
index f8f42c1..34bdf83 100644
--- a/Predator-Prey/agents/pos_cac_fo/agent.py
+++ b/Predator-Prey/agents/pos_cac_fo/agent.py
@@ -18,6 +18,7 @@ from agents.pos_cac_fo.dq_network import *
 from agents.pos_cac_fo.replay_buffer import *
 from agents.evaluation import Evaluation
 
+import time
 import logging
 import config
 
@@ -45,7 +46,7 @@ class Agent(object):
 
         self._name = name
         self.update_cnt = 0
-        self.target_update_period = 10000
+        self.target_update_period = 100
 
         self.df = FLAGS.df
         self.lr = FLAGS.lr
@@ -55,8 +56,10 @@ class Agent(object):
         my_graph = tf.Graph()
 
         with my_graph.as_default():
-            self.sess = tf.Session(graph=my_graph, config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))
-            self.q_network = DQNetwork(self.sess, self._state_dim, self._action_dim_single, self._n_predator) 
+            self.sess = tf.Session(graph=my_graph, config=tf.ConfigProto(log_device_placement=True,gpu_options=tf.GPUOptions(allow_growth=True)))
+            #with tf.device('/gpu:0'):
+            self.q_network = DQNetwork(self.sess, self._state_dim, self._action_dim_single, self._n_predator)
+            self.writer = tf.summary.FileWriter("QTRAN_graph", self.sess.graph)
             self.sess.run(tf.global_variables_initializer())
             self.saver = tf.train.Saver()
             if FLAGS.load_nn:
@@ -94,9 +97,17 @@ class Agent(object):
         s_n = self.state_to_index(state_n)
         r = np.sum(reward)
 
+        #storing sample of actions(both predator and prey) at each timestep
+        store_sample_time = time.clock()
         self.store_sample(s, a, r, s_n, done)
+        #print "store_sample start time ", store_sample_time, "total time", time.clock() - store_sample_time
 
+        '''
+        #instead of training network once per each step,train them once per episode with larger batch size
+        update_network_time = time.clock()
         self.update_network()
+        print "update_network start time ", update_network_time, "total time", time.clock() - update_network_time
+        '''
         return 0
 
     def store_sample(self, s, a, r, s_n, done):
@@ -105,12 +116,14 @@ class Agent(object):
 
     def update_network(self):
         self.update_cnt += 1
+        #donot update q network from replay buffer if replay buffer size is less
         if len(self.replay_buffer.replay_memory) < FLAGS.pre_train_step*minibatch_size:
             return 0
 
         minibatch = self.replay_buffer.sample_from_memory()
+        training_qnet_time = time.clock()
         self.q_network.training_qnet(minibatch)
-
+        #print "q_network training time ", training_qnet_time, "total time", time.clock() - training_qnet_time
 
         if self.update_cnt % self.target_update_period == 0:
             self.q_network.training_target_qnet()
diff --git a/Predator-Prey/agents/pos_cac_fo/dq_network.py b/Predator-Prey/agents/pos_cac_fo/dq_network.py
index 2db356a..e029274 100644
--- a/Predator-Prey/agents/pos_cac_fo/dq_network.py
+++ b/Predator-Prey/agents/pos_cac_fo/dq_network.py
@@ -5,6 +5,7 @@ import random
 import config
 import os
 from agents.evaluation import Evaluation
+import time
 
 FLAGS = config.flags.FLAGS
 
@@ -57,6 +58,7 @@ class DQNetwork(object):
         # indicators (go into target computation)
         self.is_training_ph = tf.placeholder(dtype=tf.bool, shape=())  # for dropout
 
+        #q_network is Q(jt) network , Inputs: state,action gives the Q(jt) factor
         with tf.variable_scope('q_network'):
             if FLAGS.algorithm == "vdn":
                 self.q_network, self.actor_network, self.v, self.qp  = self.generate_VDN(self.s_in, self.a_in, True)
@@ -95,10 +97,12 @@ class DQNetwork(object):
                 with tf.device('/cpu:0'): 
                     self.cpu_q_network, self.cpu_actor_network, self.cpu_v, self.cpu_qp  = self.generate_PQMIX6(self.s_in, self.a_in, True)
             elif FLAGS.algorithm == "pqmix7": #QTRAN-base
-                self.q_network, self.actor_network, self.v, self.p, self.qp  = self.generate_PQMIX7(self.s_in, self.a_in, True)
-                with tf.device('/cpu:0'): 
-                    self.cpu_q_network, self.cpu_actor_network, self.cpu_v, self.cpu_p, self.cpu_qp  = self.generate_PQMIX7(self.s_in, self.a_in, True)
+                with tf.device('/gpu:0'):
+                    self.q_network, self.actor_network, self.v, self.p, self.qp  = self.generate_PQMIX7(self.s_in, self.a_in, True)
+                #with tf.device('/gpu:0'):
+                #    self.cpu_q_network, self.cpu_actor_network, self.cpu_v, self.cpu_p, self.cpu_qp  = self.generate_PQMIX7(self.s_in, self.a_in, True)
 
+        #target_q_network is Q'(jt) network, Inputs:state,action output: Q'(jt) factor
         with tf.variable_scope('target_q_network'):
             if FLAGS.algorithm == "vdn":
                 self.target_q_network, self.target_actor_network, self.tv, self.tqp = self.generate_VDN(self.s_in, self.a_in, False)
@@ -137,12 +141,16 @@ class DQNetwork(object):
                 with tf.device('/cpu:0'): 
                     self.cpu_target_q_network, self.cpu_target_actor_network, self.cpu_tv, self.cpu_tqp = self.generate_PQMIX6(self.s_in, self.a_in, False)
             elif FLAGS.algorithm == "pqmix7":
-                self.target_q_network, self.target_actor_network, self.tv, self.tp, self.tqp = self.generate_PQMIX7(self.s_in, self.a_in, False)
-                with tf.device('/cpu:0'): 
-                    self.cpu_target_q_network, self.cpu_target_actor_network, self.cpu_tv, self.cpu_tp, self.cpu_tqp = self.generate_PQMIX7(self.s_in, self.a_in, False)
+                with tf.device('/gpu:0'):
+                    self.target_q_network, self.target_actor_network, self.tv, self.tp, self.tqp = self.generate_PQMIX7(self.s_in, self.a_in, False)
+                #with tf.device('/gpu:0'):
+                #    self.cpu_target_q_network, self.cpu_target_actor_network, self.cpu_tv, self.cpu_tp, self.cpu_tqp = self.generate_PQMIX7(self.s_in, self.a_in, False)
         
         self.action_onehot = tf.one_hot(self.actor_network, self.action_dim_single, on_value=1.0, off_value=0.0, axis=-1)
         self.target_action_onehot = tf.one_hot(self.target_actor_network, self.action_dim_single, on_value=1.0, off_value=0.0, axis=-1)
+
+        #optimization network is the adamomptimizer based loss minimizing network L()
+        #Inputs: y_in(y(dqn)),q_network(Q(jt)) to calculate L(td),self.p= penalty1(L(opt))+penalty2(L(nopt))
         with tf.variable_scope('optimization'):
             
             self.delta = self.y_in - self.q_network
@@ -157,10 +165,10 @@ class DQNetwork(object):
             self.merged = tf.summary.merge_all()
 
             if FLAGS.algorithm == "pqmix3" or FLAGS.algorithm == "pqmix4" or FLAGS.algorithm == "pqmix5" or FLAGS.algorithm == "pqmix7":
-                
+
                 self.clipped_error = tf.square(self.delta)
 
-                self.cost1 = tf.reduce_mean(self.clipped_error)      
+                self.cost1 = tf.reduce_mean(self.clipped_error)
                 self.clipped_p = tf.abs(self.p)
                 self.cost2 = tf.reduce_mean(self.clipped_p)
                 self.cost = 1 * self.cost1 + (self.beta_in) * self.cost2 # pqmix, 1, 0.01 0.001
@@ -179,7 +187,9 @@ class DQNetwork(object):
                 self.cost =  self.cost1 +  self.cost2
 
 
-            self.train_network = tf.train.AdamOptimizer(learning_rate, epsilon = 1.5e-4).minimize(self.cost)
+
+            with tf.device('/gpu:0'):
+                self.train_network = tf.train.AdamOptimizer(learning_rate, epsilon = 1.5e-4).minimize(self.cost)
 
         o_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_network')
         t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_q_network')
@@ -1251,6 +1261,7 @@ class DQNetwork(object):
         q_value = p_network
         value = mp_network
 
+        #penalty1 is L(opt), penalty2 is L(nopt)
         penalty1 = tf.stop_gradient(value) - tf.reduce_sum(qmax_values, reduction_indices=1, keep_dims = True)
         penalty2 = tf.stop_gradient(q_value) - tf.reduce_sum(q_values, reduction_indices=1, keep_dims = True)
             
@@ -1271,38 +1282,55 @@ class DQNetwork(object):
         return q_value, optimal_action, value, error, error
     
     def get_action(self, state_ph):
-        return self.sess.run(self.cpu_actor_network, feed_dict={self.s_in: state_ph})
+        #return self.sess.run(self.cpu_actor_network, feed_dict={self.s_in: state_ph})
+        #sravan
+        return self.sess.run(self.actor_network, feed_dict={self.s_in: state_ph})
 
+    '''
     def get_q_values(self, state_ph, action_ph):
         if FLAGS.algorithm == "pqmix":
-            target_penalty = self.sess.run(self.cpu_tp, feed_dict={self.s_in: state_ph})
-            return self.sess.run(self.cpu_q_network, feed_dict={self.s_in: state_ph,
-                                                        self.a_in: action_ph, self.p_in: target_penalty})
-
+            #target_penalty = self.sess.run(self.cpu_tp, feed_dict={self.s_in: state_ph})
+            target_penalty = self.sess.run(self.tp, feed_dict={self.s_in: state_ph})
+            #return self.sess.run(self.cpu_q_network, feed_dict={self.s_in: state_ph,
+            #                                            self.a_in: action_ph, self.p_in: target_penalty})
+            #sravan
+            return self.sess.run(self.q_network, feed_dict={self.s_in: state_ph,
+                                                                self.a_in: action_ph, self.p_in: target_penalty})
         else:
-            return self.sess.run(self.cpu_q_network, feed_dict={self.s_in: state_ph,
-                                                        self.a_in: action_ph})
-
+            #return self.sess.run(self.cpu_q_network, feed_dict={self.s_in: state_ph,
+            #                                            self.a_in: action_ph})
+            return self.sess.run(self.q_network, feed_dict={self.s_in: state_ph,
+                                                                self.a_in: action_ph})
+    '''
+    '''
     def get_qp_values(self, state_ph, action_ph):
         if FLAGS.algorithm == "pqmix":
             target_penalty = self.sess.run(self.cpu_tp, feed_dict={self.s_in: state_ph})
-            return self.sess.run(self.cpu_qp, feed_dict={self.s_in: state_ph,
-                                                        self.a_in: action_ph, self.p_in: target_penalty})
-
+            #return self.sess.run(self.cpu_qp, feed_dict={self.s_in: state_ph,
+            #                                            self.a_in: action_ph, self.p_in: target_penalty})
+            #sravan
+            return self.sess.run(self.qp, feed_dict={self.s_in: state_ph,
+                                                         self.a_in: action_ph, self.p_in: target_penalty})
         else:
-            return self.sess.run(self.cpu_qp, feed_dict={self.s_in: state_ph,
-                                                        self.a_in: action_ph})
-
+            #return self.sess.run(self.cpu_qp, feed_dict={self.s_in: state_ph,
+            #                                            self.a_in: action_ph})
+            #sravan
+            return self.sess.run(self.qp, feed_dict={self.s_in: state_ph,
+                                                         self.a_in: action_ph})
+    '''
+    '''
     def get_target_q_values(self, state_ph, action_ph):
         return self.sess.run(self.target_q_network, feed_dict={self.s_in: state_ph,
                                                                self.a_in: action_ph})
-
+    '''
     def training_qnet(self, minibatch):
+        # train the network using total loss of adamoptimizer network q_train_network
         y = []
         self.cnt += 1
 
-
+        target_qvalues_time = time.clock()
         # Get target value from target network
+        # Get target_q_values for the minibatch
         if FLAGS.algorithm == "pqmix":
             # target_action, stp = self.sess.run([self.target_action_onehot,self.tp], feed_dict={self.s_in: [data[3] for data in minibatch]})
             stp = self.sess.run(self.tp, feed_dict={self.s_in: [data[3] for data in minibatch]})
@@ -1323,9 +1351,11 @@ class DQNetwork(object):
         done = np.array([[data[4]] for data in minibatch])
         y = r + gamma * (1-done) * target_q_values
 
+        #print "target_q_values time ", target_qvalues_time, "total time", time.clock() - target_qvalues_time
 
 
 
+        train_network_sess_time =time.clock()
         if FLAGS.algorithm == "pqmix":
             c,c2, _= self.sess.run([self.cost, self.meanq, self.train_network], feed_dict={
                     self.y_in: y,
@@ -1388,7 +1418,9 @@ class DQNetwork(object):
             if self.cnt % 10000 == 0 and self.cnt >= 10000:
                 print "Penalty: ", FLAGS.penalty, "meanQ: ", self.cnt, self.TDerror2/10000, '%.3f' % self.beta
                 self.TDerror2 = 0
-        
+        #print "train_network sess start time ", train_network_sess_time, "total time", time.clock() - train_network_sess_time
+
+
 
             
     def training_target_qnet(self):
diff --git a/Predator-Prey/agents/pos_cac_fo/trainer.py b/Predator-Prey/agents/pos_cac_fo/trainer.py
index c5877fe..d8865c6 100644
--- a/Predator-Prey/agents/pos_cac_fo/trainer.py
+++ b/Predator-Prey/agents/pos_cac_fo/trainer.py
@@ -14,6 +14,7 @@ Choose action based on q-learning algorithm
 """
 
 import numpy as np
+np.set_printoptions(suppress=True)
 import tensorflow as tf
 from agents.pos_cac_fo.agent import Agent
 from agents.simple_agent import RandomAgent as NonLearningAgent
@@ -23,6 +24,7 @@ from agents.simple_agent import ActiveAgent as AcAgent
 import logging
 import config
 from envs.gui import canvas
+import time
 
 FLAGS = config.flags.FLAGS
 logger = logging.getLogger("Agent")
@@ -44,27 +46,128 @@ class Trainer(object):
         self._eval = Evaluation()
         self._n_predator = FLAGS.n_predator
         self._n_prey = FLAGS.n_prey
-        self._agent_profile = self._env.get_agent_profile()
-        self._agent_precedence = self._env.agent_precedence
+        self._agent_profile = self._env[0].get_agent_profile()
+        self._agent_precedence = self._env[0].agent_precedence
 
         self._agent = Agent(self._agent_profile["predator"]["act_dim"], self._agent_profile["predator"]["obs_dim"][0])
         self._prey_agent = AcAgent(5)
 
         self.epsilon = 1.0
+
         if FLAGS.load_nn:
             self.epsilon = epsilon_min
 
         if FLAGS.gui:
             self.canvas = canvas.Canvas(self._n_predator, self._n_prey, FLAGS.map_size)
             self.canvas.setup()
-    def learn(self):
+    def learn_parallel(self):
 
         step = 0
-        episode = 0
-        print_flag = False
+        episode = 1 #episode to start
+        print_flag = True
         count = 1
+        while step < training_step:
+            # parallelizing episodes start
+            start_time = time.clock()
+
+            total_reward = np.zeros(FLAGS.parallel_episodes)
+            total_reward_pos = np.zeros(FLAGS.parallel_episodes)
+            total_reward_neg = np.zeros(FLAGS.parallel_episodes)
+            obs_parallel = [None]*FLAGS.parallel_episodes
+            state_parallel = [None]*FLAGS.parallel_episodes
+            episode_done_parallel = np.zeros(FLAGS.parallel_episodes,dtype=bool)
+
+            episode_start = episode
+            ep_step = 1 #episode step to start
+            parallel_episode_no = 1
+            while parallel_episode_no <=FLAGS.parallel_episodes:
+                obs = self._env[parallel_episode_no-1].reset()
+                obs_parallel[parallel_episode_no-1]=obs
+                #obs_parallel.append(obs)
+                state = self._env[parallel_episode_no-1].get_full_encoding()[:, :, 2]
+                state_parallel[parallel_episode_no-1]= state
+                #state_parallel.append(state)
+                episode += 1
+                parallel_episode_no += 1
+
+
+            while ep_step <= FLAGS.max_step:
+                #print "episode_start" , episode_start, "ep_step", ep_step
+                episode = episode_start
+                parallel_episode_no = 1
+
+                #self._env[parallel_episode_no-1].render()
+
+                #print "getting action"
+                action_parallel = self.get_action_parallel(obs_parallel, step, state_parallel)
+                #print "got action"
+                obs_n_parallel = [None] * FLAGS.parallel_episodes
+                state_n_parallel = [None] * FLAGS.parallel_episodes
+                reward_parallel = [None]*(FLAGS.parallel_episodes)
+                done_single_parallel = [None]*(FLAGS.parallel_episodes)
+
+                while parallel_episode_no <=FLAGS.parallel_episodes:
+                    if episode_done_parallel[parallel_episode_no-1]==False:
+                        obs_n, reward, done, info = self._env[parallel_episode_no-1].step(action_parallel[parallel_episode_no-1])
+                        obs_n_parallel[parallel_episode_no-1]=obs_n
+                        #obs_n_parallel.append(obs_n)
+                        reward_parallel[parallel_episode_no-1]=reward
+                        #reward_parallel.append(reward)
+                        state_n = self._env[parallel_episode_no-1].get_full_encoding()[:, :, 2]
+                        state_n_parallel[parallel_episode_no-1]=state_n
+                        #state_n_parallel.append(state_n)
+                        done_single = sum(done) >0
+                        done_single_parallel[parallel_episode_no-1]=(done_single)
+                        self.train_agents(state_parallel[parallel_episode_no-1],
+                                          action_parallel[parallel_episode_no-1],
+                                          reward, state_n, done_single)
+
+                        total_reward[parallel_episode_no-1] += np.sum(reward)
+                        if np.sum(reward) >= 0:
+                            total_reward_pos[parallel_episode_no-1] += np.sum(reward)
+                        else:
+                            total_reward_neg[parallel_episode_no-1] += np.sum(reward)
+                        if is_episode_done(done, step) or ep_step >= FLAGS.max_step:
+                            # print step, ep_step, total_reward
+                            if print_flag and episode % FLAGS.eval_step == 1:
+                                print "[train_ep %d]" % (episode), "\treward", total_reward[parallel_episode_no-1] #total_reward_pos, total_reward_neg
+                            #print "TRAINING TIME for ", episode, " no: training episode ", (
+                            #    step), "done training steps  (sec)", time.clock() - start_time
+                            episode_done_parallel[parallel_episode_no - 1] = is_episode_done(done, step)
+                    episode += 1
+                    parallel_episode_no += 1
+
+                obs_parallel = obs_n_parallel
+                state_parallel = state_n_parallel
+
+                ep_step += 1
+                step += FLAGS.parallel_episodes
+
+                if episode % 1 == 0:
+                    update_network_time = time.clock()
+                    self._agent.update_network()
+                    #print "update_network start time ", update_network_time, "total time", time.clock() - update_network_time
+
+            episode = episode_start + FLAGS.parallel_episodes
+
+
+            self.test_parallel(episode)
 
+
+            # parallelizing episodes end
+        self._eval.summarize()
+        self._agent.writer.close()
+
+
+
+    def learn(self):
+
+        step = 0
+        episode = 0 #episode to start
+        print_flag = True
+        count = 1
         while step < training_step:
+
             episode += 1
             ep_step = 0
             obs = self._env.reset()
@@ -73,15 +176,21 @@ class Trainer(object):
             total_reward_pos = 0
             total_reward_neg = 0
             self.random_action_generator()
+            start_time = time.clock()
             while True:
                 step += 1
                 ep_step += 1
+                get_action_time = time.clock()
                 action = self.get_action(obs, step, state)
+                #get_action takes 0.003 seconds for each step
+                #print "get_action start time ",get_action_time, "total time", time.clock()-get_action_time
                 obs_n, reward, done, info = self._env.step(action)
                 state_n = self._env.get_full_encoding()[:, :, 2]
                 done_single = sum(done) > 0
 
+                train_agents_time = time.clock()
                 self.train_agents(state, action, reward, state_n, done_single)
+                #print "train_agents start time ",train_agents_time, "total time", time.clock()-train_agents_time
                 obs = obs_n
                 state = state_n
                 total_reward += np.sum(reward)
@@ -94,13 +203,26 @@ class Trainer(object):
                     # print step, ep_step, total_reward
                     if print_flag and episode % FLAGS.eval_step == 1:
                         print "[train_ep %d]" % (episode), "\treward", total_reward_pos, total_reward_neg
+                    print "TRAINING TIME for ", episode, " no: training episode ", (
+                        step), "done training steps  (sec)", time.clock() - start_time
                     break
 
+
+            #update network only once per 10 episodes
+            if episode%1 ==0:
+                update_network_time = time.clock()
+                self._agent.update_network()
+                print "update_network start time ", update_network_time, "total time", time.clock() - update_network_time
+
             if episode % FLAGS.eval_step == 0:
                 self.test(episode)
 
+
         self._eval.summarize()
-    
+        self._agent.writer.close()
+
+
+
     def random_action_generator(self):
         rand_unit = np.random.uniform(size = (FLAGS.n_predator, 5))
         self.rand = rand_unit / np.sum(rand_unit, axis=1, keepdims=True)
@@ -115,6 +237,7 @@ class Trainer(object):
 
         action_list = self._agent.act(state)
         for i in range(self._n_predator):
+            #choose a random action if step < FLAGS.m_size * FLAGS.pre_train_step and epsilon greedy
             if train and (step < FLAGS.m_size * FLAGS.pre_train_step or np.random.rand() < self.epsilon):
                 action = np.random.choice(5)
                 act_n.append(action)
@@ -130,9 +253,195 @@ class Trainer(object):
 
         return np.array(act_n, dtype=np.int32)
 
+    def get_action_parallel(self, obs_parallel, step, state_parallel, train=True):
+        act_n = []
+        if train == True:
+            self.epsilon = max(self.epsilon - ((epsilon_dec)*FLAGS.parallel_episodes), epsilon_min)
+
+        # Action of predator
+        s_parallel = [None]*FLAGS.parallel_episodes
+        for sravan in range(FLAGS.parallel_episodes):
+            s = self._agent.state_to_index(state_parallel[sravan])
+            s_parallel[sravan]=s
+
+        action_parallel = self._agent.sess.run(self._agent.q_network.actor_network,feed_dict={self._agent.q_network.s_in: s_parallel})
+
+        action_n_parallel = action_parallel.tolist()#[[None]*self._n_predator for i in range(FLAGS.parallel_episodes)]#[[None]*self._n_predator]*FLAGS.parallel_episodes
+        for parallel_episode in range(FLAGS.parallel_episodes):
+            for predator in range(self._n_predator):
+                #choose a random action if step < FLAGS.m_size * FLAGS.pre_train_step and epsilon greedy
+                if train and (step < FLAGS.m_size * FLAGS.pre_train_step or np.random.rand() < self.epsilon):
+                    action = np.random.choice(5)
+                    #act_n.append(action)
+
+                    action_n_parallel[parallel_episode][predator] = action
+                #else:
+                #    if train:
+                #        print "taking correct non random action"
+
+                #else:
+                #    act_n.append(action_list[i])
+            for i in range(FLAGS.n_prey):
+                (action_n_parallel[parallel_episode]).append(self._prey_agent.act(state_parallel[parallel_episode], i))
+
+        return action_n_parallel
+
+        #stp = self.sess.run(self.tp, feed_dict={self.s_in: [data[3] for data in minibatch]})
+        '''
+        action_list = self._agent.act(state)
+
+        def act(self, state):
+    
+            predator_rand = np.random.permutation(FLAGS.n_predator)
+            prey_rand = np.random.permutation(FLAGS.n_prey)      
+    
+            s = self.state_to_index(state)
+        
+            action = self.q_network.get_action(s[None])[0]
+
+            def get_action(self, state_ph):
+                #return self.sess.run(self.cpu_actor_network, feed_dict={self.s_in: state_ph})
+                #sravan
+                return self.sess.run(self.actor_network, feed_dict={self.s_in: state_ph})
+        
+        
+            return action
+        
+        
+
+        for i in range(self._n_predator):
+            #choose a random action if step < FLAGS.m_size * FLAGS.pre_train_step and epsilon greedy
+            if train and (step < FLAGS.m_size * FLAGS.pre_train_step or np.random.rand() < self.epsilon):
+                action = np.random.choice(5)
+                act_n.append(action)
+            else:
+                act_n.append(action_list[i])
+        
+
+
+        # Action of prey
+        for i in range(FLAGS.n_prey):
+            act_n.append(self._prey_agent.act(state, i))
+        # act_n[1] = 2
+        
+
+        return np.array(act_n, dtype=np.int32)
+        '''
+
     def train_agents(self, state, action, reward, state_n, done):
         self._agent.train(state, action, reward, state_n, done)
 
+    def test_parallel(self, curr_ep=None):
+        step = 0
+        episode = 1
+
+        test_flag = FLAGS.kt
+        sum_reward = 0
+        sum_reward_pos = 0
+        sum_reward_neg = 0
+        while step < testing_step:
+            #parallelizing start
+            start_time = time.clock()
+
+            total_reward = np.zeros(FLAGS.parallel_episodes)
+            total_reward_pos = np.zeros(FLAGS.parallel_episodes)
+            total_reward_neg = np.zeros(FLAGS.parallel_episodes)
+
+            obs_parallel = [None]*FLAGS.parallel_episodes
+            state_parallel = [None]*FLAGS.parallel_episodes
+            episode_done_parallel = np.zeros(FLAGS.parallel_episodes,dtype=bool)
+
+
+            episode_start = episode
+            ep_step = 1 #episode step to start
+            parallel_episode_no = 1
+
+            while parallel_episode_no <=FLAGS.parallel_episodes:
+                obs = self._env[parallel_episode_no-1].reset()
+                obs_parallel[parallel_episode_no-1]=obs
+                #obs_parallel.append(obs)
+                state = self._env[parallel_episode_no-1].get_full_encoding()[:, :, 2]
+                state_parallel[parallel_episode_no-1]= state
+                #state_parallel.append(state)
+                episode += 1
+                parallel_episode_no += 1
+
+            while ep_step <= FLAGS.max_step:
+                #print "testing episode_start" , episode_start, "testing ep_step", ep_step
+
+                episode = episode_start
+                parallel_episode_no = 1
+
+                #print "getting test action"
+                action_parallel = self.get_action_parallel(obs_parallel, step, state_parallel,False)
+                #print "got test action"
+                obs_n_parallel = [None] * FLAGS.parallel_episodes
+                state_n_parallel = [None] * FLAGS.parallel_episodes
+                reward_parallel = [None]*(FLAGS.parallel_episodes)
+                done_single_parallel = [None]*(FLAGS.parallel_episodes)
+
+                while parallel_episode_no <=FLAGS.parallel_episodes:
+                    if episode_done_parallel[parallel_episode_no-1]==False:
+                        obs_n, reward, done, info = self._env[parallel_episode_no-1].step(action_parallel[parallel_episode_no-1])
+                        obs_n_parallel[parallel_episode_no-1]=obs_n
+                        #obs_n_parallel.append(obs_n)
+                        reward_parallel[parallel_episode_no-1]=reward
+                        #reward_parallel.append(reward)
+                        state_n = self._env[parallel_episode_no-1].get_full_encoding()[:, :, 2]
+                        state_n_parallel[parallel_episode_no-1]=state_n
+                        #state_n_parallel.append(state_n)
+                        done_single = sum(done) >0
+                        done_single_parallel[parallel_episode_no-1]=(done_single)
+
+                        '''parallelize canvas here
+                        state_next = state_to_index(state_n)
+                        if FLAGS.gui:
+                            self.canvas.draw(state_next, done, "Score:" + str(total_reward) + ", Step:" + str(ep_step))
+                        '''
+
+                        total_reward[parallel_episode_no-1] += np.sum(reward)
+                        if np.sum(reward) >= 0:
+                            total_reward_pos[parallel_episode_no-1] += np.sum(reward)
+                        else:
+                            total_reward_neg[parallel_episode_no-1] += np.sum(reward)
+                        if is_episode_done(done, step,"test") or ep_step >= FLAGS.max_step:
+                            # print step, ep_step, total_reward
+                            #print "TESTING TIME for ", episode, "no: test episode ", step, "done testing steps (sec)", time.clock() - start_time
+
+                            '''parallelize canvas
+                            if FLAGS.gui:
+                                self.canvas.draw(state_next, done, "Hello",
+                                                 "Score:" + str(total_reward) + ", Step:" + str(ep_step))
+                            '''
+
+                            episode_done_parallel[parallel_episode_no - 1] = is_episode_done(done, step)
+                    episode += 1
+                    parallel_episode_no += 1
+
+                obs_parallel = obs_n_parallel
+                state_parallel = state_n_parallel
+
+                ep_step += 1
+                step += FLAGS.parallel_episodes
+
+            episode = episode_start + FLAGS.parallel_episodes
+
+            sum_reward += np.sum(total_reward)
+            sum_reward_pos += np.sum(total_reward_pos)
+            sum_reward_neg += np.sum(total_reward_neg)
+
+
+            #parallelizing end
+
+
+        if FLAGS.scenario == "pursuit":
+            print "Test result: Average steps to capture: ", curr_ep, float(step) / episode
+            self._eval.update_value("training result: ", float(step) / episode, curr_ep)
+        elif FLAGS.scenario == "endless" or FLAGS.scenario == "endless2" or FLAGS.scenario == "endless3":
+            print "Average reward:", FLAGS.penalty, curr_ep, sum_reward / episode, sum_reward_pos / episode, sum_reward_neg / episode
+            self._eval.update_value("training result: ", sum_reward / episode, curr_ep)
+            self._agent.logging(sum_reward/episode, curr_ep * 100)
+
     def test(self, curr_ep=None):
 
         step = 0
@@ -153,6 +462,7 @@ class Trainer(object):
             total_reward_neg = 0
 
             ep_step = 0
+            start_time = time.clock()
 
             while True:
 
@@ -187,6 +497,7 @@ class Trainer(object):
 
 
                 if is_episode_done(done, step, "test") or ep_step >= FLAGS.max_step:
+                    print "TESTING TIME for ", episode , "no: test episode " , step , "done testing steps (sec)", time.clock() - start_time
 
                     if FLAGS.gui:
                         self.canvas.draw(state_next, done, "Hello", "Score:" + str(total_reward) + ", Step:" + str(ep_step))
@@ -195,6 +506,7 @@ class Trainer(object):
             sum_reward += total_reward
             sum_reward_pos += total_reward_pos
             sum_reward_neg += total_reward_neg
+
         if FLAGS.scenario =="pursuit":
             print "Test result: Average steps to capture: ", curr_ep, float(step)/episode
             self._eval.update_value("training result: ", float(step)/episode, curr_ep)
diff --git a/Predator-Prey/config.py b/Predator-Prey/config.py
index 30d29d7..b071291 100644
--- a/Predator-Prey/config.py
+++ b/Predator-Prey/config.py
@@ -10,7 +10,7 @@ import agents.config_agents as config_agent
 flags = tf.flags
 
 flags.DEFINE_integer("seed", 0, "Random seed number")
-flags.DEFINE_string("folder", "default", "Result file folder name")
+flags.DEFINE_string("folder", "sravan-run", "Result file folder name")
 flags.DEFINE_string("comment", "None",
                     "Additional Comments")
 flags.DEFINE_boolean("gui", False, "Activate GUI")
diff --git a/Predator-Prey/envs/config_env.py b/Predator-Prey/envs/config_env.py
index d16539b..3b3140a 100644
--- a/Predator-Prey/envs/config_env.py
+++ b/Predator-Prey/envs/config_env.py
@@ -8,20 +8,21 @@ def config_env(_flags):
     
 
     # Scenario
-    flags.DEFINE_string("scenario", "pursuit", "Scenario")
+    flags.DEFINE_string("scenario", "endless3", "Scenario")
     flags.DEFINE_integer("n_predator", 2, "Number of predators")
-    flags.DEFINE_integer("n_prey1", 1, "Number of preys 1")
+    flags.DEFINE_integer("n_prey1", 0, "Number of preys 1")
     flags.DEFINE_integer("n_prey2", 1, "Number of preys 2")
-    flags.DEFINE_integer("n_prey", 2, "Number of preys")
+    flags.DEFINE_integer("n_prey", 1, "Number of preys")
     # Observation
     flags.DEFINE_integer("history_len", 1, "How many previous steps we look back")
 
     # core
-    flags.DEFINE_integer("map_size", 3, "Size of the map")
+    flags.DEFINE_integer("map_size", 5, "Size of the map")
+    flags.DEFINE_integer("parallel_episodes",100,"Number of episodes to be parallely run")
     flags.DEFINE_float("render_every", 1000, "Render the nth episode")
 
     # Penalty
-    flags.DEFINE_integer("penalty", 1, "reward penalty")
+    flags.DEFINE_integer("penalty", 5 , "reward penalty")
 
 def get_filename():
     import config
diff --git a/Predator-Prey/main.py b/Predator-Prey/main.py
index 26e4d02..f1818bd 100644
--- a/Predator-Prey/main.py
+++ b/Predator-Prey/main.py
@@ -33,10 +33,19 @@ if __name__ == '__main__':
     logger_env = logging.getLogger('GridMARL')
     logger_agent = logging.getLogger('Agent')
 
+    '''
     # === Program start === #
     # Load environment
     env = make_env.make_env(FLAGS.scenario)
     logger_env.info('GridMARL Start with %d predator(s) and %d prey(s)', FLAGS.n_predator, FLAGS.n_prey)
+    '''
+
+    #parallel environments
+    env = []
+    for _ in range(FLAGS.parallel_episodes):
+        env.append(make_env.make_env(FLAGS.scenario))
+    logger_env.info('GridMARL Start with %d predator(s) and %d prey(s)', FLAGS.n_predator, FLAGS.n_prey)
+    #parallel environments end
 
     # Load trainer
     logger_agent.info('Agent: {}'.format(FLAGS.agent))
@@ -47,9 +56,10 @@ if __name__ == '__main__':
     # start learning
     if FLAGS.train:
         start_time = time.time()
-        trainer.learn()
+        trainer.learn_parallel()
         finish_time = time.time()
         # trainer.test()
         print "TRAINING TIME (sec)", finish_time - start_time
     else:
         trainer.test()
+
diff --git a/Predator-Prey/readme b/Predator-Prey/readme
index b4f4df5..ce5caba 100644
--- a/Predator-Prey/readme
+++ b/Predator-Prey/readme
@@ -10,4 +10,32 @@ python main.py --scenario endless3 --n_predator 2 --n_prey1 0 --n_prey2 1 --n_pr
 
 python main.py --scenario endless3 --n_predator 4 --n_prey1 0 --n_prey2 2 --n_prey 2 --map_size 7 --agent pos_cac_fo --training_step 6000000 --testing_step 10000 --max_step 100 --b_size 1000000 --df 0.99 --eval_step 100 --algorithm $algorithm --lr 0.0005 --seed 0 --penalty 5 --comment 427 &
 
+--scenario endless3
+--n_predator 2
+--n_prey1 0
+--n_prey2 1
+--n_prey 1
+--map_size 5
+--agent pos_cac_fo
+--training_step 3000000
+--testing_step 10000
+--max_step 100
+--b_size 600000
+--df 0.99
+--eval_step 100
+--lr 0.0005 --seed 0 --penalty 5 --comment 215
+extra hyperparameter self.target_update_period
+epsilon_dec,epsilon_min, epsilon in init in trainer.py
+
+high self.target_update_period can decrease instability in convergence of q_network
+penalty is divided by 10 internally
+initially due to high epsilon training results are lower than testing results,because of exploration
+
+instability causes:
+learning_rate of adamoptimizer
+target_update_period
+batch size
+
+https://stackoverflow.com/questions/37044600/sudden-drop-in-accuracy-while-training-a-deep-neural-net
+lower learning rate to avoid sudden drop in score
 
